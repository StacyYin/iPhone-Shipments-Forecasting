---
title: "iPhone Shippments Forecasting"
author: "Mduduzi Langwenya, Shengchen Fu, Tianyi Zhou, Shihan Yin"
date: "May 2, 2019"
output: html_document
---

### Part I Introduction 

For this project, we analyzed global iPhone quarterly shipments from the second half of 2007 to the end of 2018. Apple introduced the iPhone in 2007, and its global sales grew from 1.4 million units to over 200 million units in 2018. Apple stopped reporting the breakdown of iPhone shipments in its quarterly financial reports published in 2019 and beyond. 

We analyzed the series for trends, seasonality and autocorrelations. The we raced several forecasting models to compare which one performs best. 

### Packages

```{r cars, message=FALSE, warning=FALSE}
library(readxl); library(forecast); library(urca); library(tseries); library(zoo)
```

###  Part II Trend, seasonality and autocorrelation in Apple shipments data 

We first plotted the iphone shipments against time to explore whether there is seasonality or trends in the data. The graph below suggests that there is an consistent trend over time, and that there is also strong seasonality, with a particularly strong fourth quarter each year.  This is not surprising given that seasonal patterns of  iPhone shipments are well known in the mobile technology industry.  New iPhones are released  to the public towards the end of the third quarter, just in time for the holiday shopping period.

```{r}
#load the time series
apple_shipments <- read_excel("apple shipments.xlsx",sheet = "Data")

apple.ts <- ts(apple_shipments$`iPhone Unit Sales (M)`,start=c(2007,3),freq=4)
plot(apple.ts)

```

The graph also suggests that the seasonal variation increases with the level of the series, so we decided to perform log transformation to satisfy the assumption, from most models, that variability stays constant over time. This had the effect of transforming the seasonal pattern from multiplicative to additive. The log transformation did not address the consistent upward trend, so we also performed first differencing to eliminate the trend across time.  The resulting series is shown below. 

```{r}
#focus on the growth rate
apple.ts <- diff(log(apple.ts))

#check for trend and seasonality using eyeball
plot(apple.ts)
```

We also plotted the ACF to identify the appropriate models to fit. 
```{r}
#check Autocorrelation
Acf(apple.ts)
```

There is a significant autocorrelation spike at lag 2, 4 and 8. Given that the data is quarterly, we believed that this could be an indicator of seasonality in the log transformed and first differenced iphone series. To remove this seasonal effect, we will fit models that account for seasonality (e.g. seasonal dummy variables in linear models, or additive seasonality in the holt winters exponential filter). 


### Test stationarity

```{r}
#before conducting DF test, we need to deseasonalize the data
#deseasonalize the data
app.mod <- tslm(apple.ts ~ trend + season)
appmean <- mean(apple.ts)
appdes <- app.mod$residuals + appmean

#add the trend back
app.adjusted <- appdes + app.mod$coefficients[2]*(1:T)
plot(app.adjusted)

#Dickey-Fuller on trending 
#considering that 59.5105 is larger than 9.31. We reject the null hypothesis. 
print(summary(ur.df(app.adjusted,type=c("trend"),selectlags="BIC")))

#Augmented Dicky-Fuller
#Test result indicates that the series is stationary under 0.05 confidence level
adf.test(app.adjusted)
```

So, we decide to use d=0 for diff(log(apple.ts)) in Arima models.

### Part III Models
### Training and validation

```{r}
#split the training and validation dataset
train.ts <- window(apple.ts, end = c(2016,4))
valid.ts <- window(apple.ts, start = c(2017, 1))

T <- length(apple.ts)
T1 <- length(train.ts)
stepsAhead <- length(valid.ts)
```

### 1. Naive forecast 

By investigating the plot of APPLE growth rate time series and the autocorrelation, we first decided to fit the sales growth rate on a seasonal naive model.Then we applied forecast on validation data.

This model gives 0.53 RMSE in the training period and 0.05 RMSE in the validation period. By plotting the real data, fitted value in training and predicted value in validation, it is surprising that seasonal naive model has superb accuracy for forecast in validation.

```{r}
# fit seasonal naive model
naive.lm <- snaive(train.ts, h = stepsAhead, level = 95)
accuracy(naive.lm, valid.ts)
plot(naive.lm,  ylab = "Shipments", xlab = "Time", bty = "l",xaxt="n", 
     main="seasonal naive", flty = 2)
lines(naive.lm$fitted, col="red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```

### 2. Linear Models 
For linear models, based on the plot of full data, there seems to be trend in the training period and no trend in validation period. Thus, we tried to figure if trend help with forecast.

To begin with a simple linear model, we regressed sales growth rate on the seasonality in training data  and applied forecast in validation data. In the model fitting result, This model gives 0.45 RMSE in training and 0.23 in validation.
```{r}
# fit  linear model with seasonality and no trend and perform forecasts
apple.lm <-  tslm(train.ts ~ season)
apple.lm.pred <- forecast(apple.lm, h = stepsAhead, level = 95)
summary(apple.lm)
accuracy(apple.lm.pred, valid.ts)

plot(apple.lm.pred, ylab = "Shipments",bty='l',xlab = "Time",xaxt="n", 
     main = "linear trend", flty = 2)
lines(apple.lm$fitted, col="red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```

Then by adding trend and trend^2 as predictors, we obtained a new linear model with both trend and seasonality. This quadratic model gives 0.43 RMSE in training data and 0.20 RMSE in validation data, which improves the accuracy from linear model with seasonality and no trend.
```{r}
# fit linear model with seasonality and quadratic trend model and perform forecasts
quadratic.lm <-  tslm(train.ts ~ trend + I(trend^2) + season)
quadratic.lm.pred <- forecast(quadratic.lm, h = stepsAhead, level = 0)
accuracy(quadratic.lm.pred, valid.ts)

plot(quadratic.lm.pred, ylab = "Shipments",bty='l',xlab = "Time",xaxt="n", 
     main = "quadratic trend", flty = 2)
lines(quadratic.lm$fitted, col="red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```


### 3. Exponential Filters

```{r}
# First, let ets estimate additive or multiplicative trend/seasonal filter
auto.ses <- ets(train.ts)
# Now, build forecasts for validation periods (uses no data in validation)
auto.ses.pred <- forecast(auto.ses, h=stepsAhead, level=95)
accuracy(auto.ses.pred, valid.ts)

# plot all the results
plot(auto.ses.pred, ylab="Shipments (M)", xlab="Time",  main="", flty=1)
lines(auto.ses.pred$fitted, col = "red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```


```{r}
# find an ets filter with better accuracy 
ses <- ets(train.ts, model="AAA")

# Now, build forecasts for validation periods (uses no data there)
ses.pred <- forecast(ses, h=stepsAhead, level=0.95)
accuracy(ses.pred, valid.ts)

# plot all the results
plot(ses.pred, ylab="Shipments (M)", xlab="Time",  main="", flty=1)
lines(ses.pred$fitted, col="red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```

Given that the log transformed and first differenced iPhone data still had some seasonality remaining in it, we decided to use the holt winters seasonal method to forecast the data. We preferred an additive structure given that the seasonal trend is of largely of the same magnitude especially as time increases. 

We let the ets function choose the optimal model based on minimal AIC and BIC values in the training set. It selected the (A, N, A) model specification, which assumes additive noise, no trend, and additive seasonality.  While this model had the lowest AIC and BIC values, it did not have the lowest RMSE. Instead we ran different combinations of model specifications and found that additive noise, additive trend, and additive seasonality (A, A, A) results in an small improvement in the RMSE of the validation data. We used this optimal model to generate one step and multistep forecasts. 


### 4. ARIMA Model

We conducted two models w/o trend to compare. First, we made the forecast based on the Arima model without trend. And we looked at the one-step and multiple-step forecasts seperately.

```{r}
#ARIMA without trend

#auto tune based on bic
forecast.mod <- auto.arima(train.ts,d=0,ic="bic",seasonal=TRUE)
forecast.mod
# forecast - no information from validation period
arima.no.trend <- forecast(forecast.mod, h = stepsAhead, level=95)
#accuracy in validation period
accuracy(arima.no.trend, valid.ts)

# plot all the results
plot(arima.no.trend, ylab="Shipments (M)", xlab="Time",  main="", flty=1)
lines(arima.no.trend$fitted, col="red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```

RMSE of the training set and the test set are similar, which are 0.3187852 and 0.3190056. Therefore this multiple-step ahead forecast doesn't have an overfitting issue. From the plot we got, we could see that the forecast has a good ability to forcast the up and down of shipments in the validation period. However, the range of forecasting values is much smaller than the real data, which means that it stil needs some improvement.

```{r}
#one step ahead in validation
onestep.notrend.arima <- Arima(valid.ts, model = forecast.mod)
#accuracy for one-step ahead
accuracy(onestep.notrend.arima$fitted, valid.ts)

#plot the forecasting results
plot(onestep.notrend.arima$fitted, ylab="Shipments (M)", xlab="Time",  main="",col = "red")
lines(valid.ts)
```

Surprisingly, RMSE drops significantly to 0.1444727 in the test set. What's more, from the plot, the red line, which is the forecasting result, approaches the real validation data very much, which indicates a high accuracy in the one-step forecast.

```{r, warning=FALSE, message=FALSE}
#ARIMA with trend (ARIMAX)

#check the best model parameters based on bic
forecast.mod.up <- auto.arima(train.ts,d=0,ic="bic",seasonal=TRUE,xreg = 1:T1)
#print(summary(forecast.mod.up))

#conduct best Arima model with trend on training dataset
trend.arima <- Arima(train.ts,order=c(2,0,0),seasonal = list(order = c(1,0,0), period = 4), xreg=1:T1)

# forecast - no information from validation period
valid.trend <- forecast(trend.arima, xreg = (T1+1):T, h=stepsAhead, level=0)

#accuracy in validation period
accuracy(valid.trend, valid.ts)

# plot all the results
plot(valid.trend, ylab="Shipments (M)", xlab="Time",  main="", flty=1)
lines(valid.trend$fitted, col="red")
lines(valid.ts)
abline(v=2017, col="black")
text(2014, 1.5, "Training",cex=1.25)
text(2018, 1.5, "Validation",cex=1.25)
grid()
```

Then we added the trend to the Arima model, and did multiple-step and one-step forecasts. For the multiple-step forecast, if we compare the forecast to the previous one. We could see from the plot that 
the range of predicting values is more close to the real validation data, which makes more sense.

```{r}
#one step ahead in validation
onestep.mod <- Arima(valid.ts, xreg=(T1+1):T, model=trend.arima)
#accuracy for one-step ahead
accuracy(onestep.mod$fitted, valid.ts)

#plot the forecasting results
plot(onestep.notrend.arima$fitted, ylab="Shipments (M)", xlab="Time",  main="",col = "red")
lines(valid.ts)
```

However, if we conducted the one-step forecast on this model, RMSE of this forecast(0.197993) is larger than the model without trend(0.1444727), which means that doing one-step forecast without trend has a priority to be considered in terms of accuracy.

### Choose the best model
```{r}
#DM tests to compare this two forecasts
print(dm.test(onestep.notrend.arima$residuals, onestep.mod$residuals, alternative="less"))
```

We conducted the DM test to compare two one-step forecast models. The p-value is 0.1583, which means that we can't reject the null hypothesis under 10% confidence level. The null hypothesis is that these two models have the same accuracy. From the last part in our analysis, however, we would priortize the ARMIA model without trend ARIMA(0,0,2)(1,0,0)[4].

### 5. Compare all the models
```{r echo=FALSE}
print("snaive")
print(accuracy(naive.lm, valid.ts))
print("linear model with seasonality and no trend")
print(accuracy(apple.lm.pred, valid.ts))
print("quadratic model with trend")
print(accuracy(quadratic.lm.pred, valid.ts))
print("ETS")
print(accuracy(ses.pred, valid.ts))
print("ARIMA without trend")
print(accuracy(onestep.notrend.arima$fitted, valid.ts))
print("ARIMA with trend/ ARIMAX")
print(accuracy(onestep.mod$fitted, valid.ts))

```
We can see according to rmse, our best model is snaive and arima model without trend is the second best one. Further, we applied different validation methods on our two best models to see whehter result can be improved.

### 6. Cross validation and recursive validation
```{r}
#Cross validation on snaive model
naive_cv<-function(x,h){
  naive.lm <- snaive(x, h = h, level = 0)
}
# call forecast cross val function
eCV <- tsCV(apple.ts,naive_cv, h=1)
rmseCV <- sqrt( mean( eCV^2,na.rm=TRUE))

eCVValid <- eCV[-(1:T1) ]
rmseCVValid <- sqrt( mean( eCVValid^2,na.rm=TRUE))

print(" Snaive RMSE--------------------")
accuracy(naive.lm, valid.ts)
print(" Snaive RMSE with Cross Validation --------------------")
print(sprintf("Train %f",rmseCV))
print(sprintf(" Valid %f",rmseCVValid))

```
We can see after cross validation, the test set rmse of Snaive is slightly improved and the result is 0.032.

```{r}
# recursive forecast on ARIMA without trend /ARIMA(0,0,2)(1,0,0)[4] 
# Begin recursive forecast  
rfcast <- rep(0,stepsAhead)

for (i in 0:(stepsAhead-1)) {
  # move window along one point each time
  full.train <- window(apple.ts,end=c(2016,4+i))
  # train on recursive window
  recur.model <- Arima(full.train,order=c(0,0,2),seasonal=list(order=c(1,0,0),period=4))
  # fit validation period using this model
  onestep <- Arima(valid.ts,model=recur.model)
  tempfit <- fitted(onestep)
  # forecast is the i+1 element of the fitted model
  rfcast[i+1] <- tempfit[i+1]
}

rfcast.ts <- ts(rfcast,start=c(2017,1),freq=4)


# look at model accuracy

print("ARIMA(0,0,2)(1,0,0)[4]  ")
print(accuracy(onestep.notrend.arima$fitted,valid.ts))

print("Recursive ARIMA(0,0,2)(1,0,0)[4]  ")
print(accuracy(rfcast.ts,valid.ts))

```
Although we run a recursive validation on ARIMA(0,0,2)(1,0,0)[4] model, the RMSE was not improved as we expected.

### Part IV Conclusions

We tried snaive model, linear model with seasonality and no trend, quadratic trend model, ETS(A, A, A), Arima model with/without trend, Arima model at the one-step and multiple-step forecasts combined with cross validation and recursive methods. Finally, we found Snaive to be our optimal model with a 0.047 test set RMSE, which is signigicantly lower that those of other models. Although Snaive is a relatively simple model, the performance is unexpectetly good which also suggests strong seasonality in the growth rate of iPhone shippments.

